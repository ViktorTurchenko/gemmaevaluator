# Overview
This repository contains a FastAPI application designed to evaluate essays and determine if they were composed by an AI. The application uses a pre-trained language model from Hugging Face, specifically the "google/gemma-2b-it" model. The application is containerized using Docker for ease of deployment and scalability.

# Features
- **API Endpoint:**  The application provides a /generate/ endpoint that receives a text prompt (an essay) and returns an evaluation of whether the essay was generated by AI.
- **Scalability:** The application is designed to be scalable with the ability to increase the number of workers handling incoming requests.
- **Resource Optimization:** Currently configured to run on limited CPU resources to ensure compatibility with environments where GPU resources are not available.

# Prerequisites
- Docker installed on your machine.
- An API token from Hugging Face. You will need this token to authenticate with the Hugging Face model repository.

# Running the Application
### Building the Docker Image
To build the Docker image for this application, navigate to the directory containing the Dockerfile and run the following command:

```bash
docker build -t essay-evaluator .
```
This command builds a Docker image named **essay-evaluator** based on the instructions in your Dockerfile.

### Running the Docker Container
Run the Docker container using the following command, replacing **your_huggingface_token_here** with your actual Hugging Face API token:

```bash
docker run -p 8000:8000 -e HF_TOKEN=your_huggingface_token_here essay-evaluator
```

This command starts the container and makes the FastAPI application accessible on http://localhost:8000. The API token is passed securely as an environment variable. You need the API token in order to download the tokenizer and the model from HuggingFace which then will be ran locally.

### Scalability
This application can be scaled to handle more requests by increasing the number of Gunicorn workers. To do this, adjust the -w parameter in the Docker CMD directive within the Dockerfile. Here's how you can modify it to use more workers:

```bash
CMD ["gunicorn", "main:app", "-w", "4", "-k", "uvicorn.workers.UvicornWorker", "--timeout", "0", "--bind", "0.0.0.0:8000"]
```

### Considerations
- **CPU Usage:** Increasing the number of workers will increase CPU usage. Ensure the deployment environment has sufficient CPU resources to handle the additional load.
- **Load Balancing:** For significant scaling across multiple machines or high-availability setups, consider using a load balancer in front of the Docker containers to distribute incoming traffic evenly.